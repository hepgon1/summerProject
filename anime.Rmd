---
title: "anime"
author: "Garcia.Heather"
date: "July 8, 2018"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(sqldf)
library(caret)
library(timeDate)
library(dplyr)
library(data.table)
library(tidyverse)
```

Main is on the bottom of this file. 

64M records in the file.
83K for one anime show (21)

## Process the user to anime file. 
There are 64Mil rows in the file.  We cannot read it into memory in it's entirity.  
We plan to run this with a few randomly sampled anime ids so that we can create a sample of the data. 
```{r}
prc.usr.anime.file = function(rc, wd){
  # how many rows to count and the skip for the for loop.
  rowcnt= rc
  skp = 0
  
  #How many times to run the loop based upon the size of the file
  loop.number = ceiling(66000000/rowcnt)
  
  #The files that we are going to read and write, Shortens the code below.
  usr.anime.file = paste0(wd, "/myAnimeList Dataset/extracted data/UserAnimeList.csv")
  wrt.usr.anime.file = paste0(wd,"/code/output/MycleanUserAnimeList.csv")
  
  #Read the file and process rowcnt number of rows to pull an anime from the list of users. 
  #pr[which(pr[,2] == 21),]
  
  set.seed(123)
  for (i in 1:loop.number){
    if(i==1){
      pr = read.csv(usr.anime.file, nrows = rowcnt, skip = skp, header = T)
      write.table(pr[sample(nrow(pr), 20),], wrt.usr.anime.file, append = T, sep = ",", col.names = T, row.names = F)
    }else{
      pr = read.csv(usr.anime.file, nrows = rowcnt, skip = skp, header = F)
      write.table(pr[sample(nrow(pr), 20),], wrt.usr.anime.file, append = T, sep = ",", col.names = F, row.names = F)
    }
    
    skp = skp + rowcnt
    
    print(skp) 
    #print(head(pr))
    print(nrow(pr))
  }
}

```

#Read in the file
```{r}
read.file = function (dir, f){
  file = paste0(wd, "/", dir, "/", f)
  ds = read.csv(file, sep = ",", header = T)
  return (ds)
}

```

#Join the datasets

I decided on the fields in the join becuase:
1. Removed fields that would require text mining to compare.
2. Removed fields that had near zero variance.
3. Removed id fields
4. Removed fields that meant the same thing (title fields)

```{r}
join = function(){
  joined.ds = sqldf("select a.anime_id, a.title, a.type, a.source, a.episodes,  a.duration, a.rating, a.score, a.scored_by, a.rank, a.popularity, a.members, a.related, a.favorites, a.broadcast, a.producer, a.licensor, a.studio, a.genre,  m.username, m.my_watched_episodes, m.my_score, m.my_status, u.user_watching, u.user_completed,  u.user_onhold, u.user_dropped, u.user_plantowatch, u.gender, u.join_date, u.last_online
      from ani as a 
      join myusrAni as m
        on a.anime_id = m.anime_id
      join usr as u
        on u.username = m.username")
  wrt.usr.anime.file = paste0(wd,"/code/output/joinedDS.csv")
  write.table(joined.ds, wrt.usr.anime.file, append = F, sep = ",", col.names = T, row.names = F)
  return(joined.ds)
}
```

#Main

5654000 records processed.
```{r}
setwd("D:/Data Analytic Applications/anime")
wd = "D:/Data Analytic Applications/anime"

prc.usr.anime.file(2000, wd)
```
```{r}
myusrAni = read.file("/code/output/", "MycleanUserAnimeList.csv")
nrow(myusrAni)

ani = read.file("/myAnimeList Dataset/extracted data/", "AnimeList.csv")
head(ani)

usr = read.file("/myAnimeList Dataset/extracted data/", "UserList.csv")
head(usr)

```

#Join the datasets together

Join the datasets and check the fields.  I have removed a lot of fields. 
```{r}
all_ds = join()
```

Look at the structure and the skewness of any numeric type column
```{r}
str(all_ds)
summary(all_ds)
colnames(all_ds[sapply(all_ds, class) == "integer"])
skewness(all_ds[,c(colnames(all_ds[sapply(all_ds, class) == "integer"]))])

colnames(all_ds[sapply(all_ds, class) == "numeric"])

histogram(all_ds$score, all_ds)
```

#Evaluation of the sample dataset

If we get a random sample from the User to Anime list then we see a pretty nice sample of number of times a users did a score, the number of times the anime show occurs in the list and the distribution of the scores are reasonable. (More high than low, greater counts in the high range. 
```{r}
sqldf("select title, count(title) as cnt
      from all_ds
      group by title
      order by cnt desc ")
sqldf("select username, count(username) as cnt
      from all_ds
      group by username
      order by cnt desc ")
sqldf("select my_score, count(my_score) as cnt
      from all_ds
      group by my_score
      order by my_score desc ")
sqldf("select username, anime_id, my_score, count(*) as cnt
      from all_ds
      group by username, anime_id, my_score
      having cnt > 1
      order by cnt desc ")
sqldf("select username, anime_id, count(anime_id) as cnt
      from all_ds
      group by username, anime_id
      having cnt > 1
      order by anime_id, cnt desc ")

```

#Exploration
What does the animeList and the UserList look like?
```{r}
str(ani[c("anime_id", "title", "type", "source", "episodes", "duration", "rating", "score","scored_by", "rank", "popularity", "members", "related", "favorites", "premiered","broadcast", "producer", "licensor", "studio", "genre")])
summary(ani[c("anime_id", "title", "type", "source", "episodes", "aired_string", "duration", "rating", "score","scored_by", "rank", "popularity", "members", "related", "favorites", "premiered","broadcast", "producer", "licensor", "studio", "genre")])

str(usr)
summary(usr[, c("user_watching", "user_completed", "user_onhold", "user_dropped", "user_plantowatch", "gender", "join_date", "last_online")])

summary(all_ds)
summary(myusrAni)

#nearZeroVar check
colnames(ani[, c(nearZeroVar(ani))])
colnames(usr[, c(nearZeroVar(usr))])
colnames(myusrAni[,c(nearZeroVar(myusrAni))])

```

# Alterations

Convert the multiple level factors into less than 4, if possible.

1. Convert last_online to 
Blank or 1900 into Not reported or 1900
Greater than 1900 to 2017 into Before 2018
Equal to 2018 into 2018
```{r}
sqldf("select yr.yr, count(1) as cnt
      from (select substr(last_online, 1,4) as yr
        from all_ds) as yr
      group by yr.yr")
all_ds$lst_online_yr = substr(all_ds$last_online, 1,4)
head(all_ds$lst_online_yr)
all_ds$lst_online_yr[all_ds$lst_online_yr=="1900" | all_ds$lst_online_yr== ""] <- "Not Reported or 1900"
all_ds$lst_online_yr[all_ds$lst_online_yr > "1900" & all_ds$lst_online_yr <= "2017"] <- "Before 2018"

all_ds$lst_online_yr[all_ds$lst_online_yr=="2018"] <- "2018"
sqldf("select lst_online_yr, count(1) as cnt
      from all_ds
      group by lst_online_yr
      order by cnt desc ")

```
Convert join_date to:
3 generations.
Early - before 2009
Middle = 2009 - 2012
Late = 2012 - current
```{r}
sqldf("select yr.yr, count(1) as cnt
      from (select substr(join_date, 1,4) as yr
        from all_ds) as yr
      group by yr.yr")
all_ds$join_yr = substr(all_ds$join_date, 1,4)
head(all_ds$join_yr)
all_ds$join_yr[all_ds$join_yr >= "2012" ] = "Late"
all_ds$join_yr[all_ds$join_yr < "2009" | all_ds$join_yr== ""] = "Early"
all_ds$join_yr[all_ds$join_yr >= "2009" & all_ds$join_yr < "2012" ] = "Middle"

sqldf("select join_yr, count(1) as cnt
      from all_ds
      group by join_yr
      order by cnt desc ")

```

Location
Decided not to do location no real insight gained since the groupings are very small.
```{r}
# sqldf("select substr(location, 1,2) loc, count(1) as cnt
#         from all_ds
#       group by loc  ")
# all_ds$loc_grp = substr(all_ds$location, 1,15)
# head(all_ds$loc)
# all_ds$loc_grp[all_ds$loc_grp== ""] = "Undisclosed"
# all_ds$loc_grp[all_ds$loc_grp== grep("*[:digit:]",all_ds$loc_grp, value = T)] = "Make Believe"
# all_ds$loc_grp[all_ds$loc_grp== grep("*[:punct:]",all_ds$loc_grp, value = T)] = "Make Believe"
# all_ds$loc_grp[str_to_lower(all_ds$loc_grp) %like% 'omew'] = "Make Believe"
# 
# sqldf("select loc_grp, count(1) as cnt
#       from all_ds
#       group by loc
#       order by loc
#       desc ")


```

Gender
```{r}
all_ds$gnd_grp = all_ds$gender
all_ds$gnd_grp[all_ds$gnd_grp == ""] = "Non-Binary"
sqldf("Select gnd_grp, count(1) as cnt
      from all_ds
      group by gnd_grp")

```

source
```{r}
all_ds$anime_src = all_ds$source

all_ds$anime_src[all_ds$anime_src == "Novel"] = "Book"
all_ds$anime_src[all_ds$anime_src == "Light novel"] = "Book"
all_ds$anime_src[all_ds$anime_src == "Picture book"] = "Book"
all_ds$anime_src[all_ds$anime_src == "Visual novel"] = "Book"
all_ds$anime_src[all_ds$anime_src == "Unknown"] = "Other"
all_ds$anime_src[all_ds$anime_src == "Music"] = "Other"
all_ds$anime_src[all_ds$anime_src == "Radio"] = "Other"
all_ds$anime_src[all_ds$anime_src == "Card game"] = "Game"
all_ds$anime_src[all_ds$anime_src == "Web manga"] = "Manga"
all_ds$anime_src[all_ds$anime_src == "4-koma manga"] = "Manga"
all_ds$anime_src[all_ds$anime_src == "Digital manga"] = "Manga"
sqldf("Select anime_src, count(1) as cnt
      from all_ds
      group by anime_src")

```


duration
```{r}
all_ds$anime_len = substr(all_ds$duration, 1,3)
all_ds$anime_len[all_ds$anime_len == "1 h" ] = "hour"
all_ds$anime_len[all_ds$anime_len == "2 h" ] = "movie"
all_ds$anime_len[all_ds$anime_len == "2 m" ] = "1-20"
all_ds$anime_len[all_ds$anime_len == "1 m" ] = "1-20"
all_ds$anime_len[all_ds$anime_len == "3 m" ] = "1-20"
all_ds$anime_len[all_ds$anime_len == "4 m" ] = "1-20"
all_ds$anime_len[all_ds$anime_len == "5 m" ] = "1-20"
all_ds$anime_len[all_ds$anime_len == "6 m" ] = "1-20"
all_ds$anime_len[all_ds$anime_len == "7 m" ] = "1-20"
all_ds$anime_len[all_ds$anime_len == "8 m" ] = "1-20"
all_ds$anime_len[all_ds$anime_len == "9 m" ] = "1-20"

all_ds$anime_len[all_ds$anime_len >= "1" & all_ds$anime_len < "21" ] = "1-20"
all_ds$anime_len[all_ds$anime_len >= "21" & all_ds$anime_len < "41" ] = "21-40"
all_ds$anime_len[all_ds$anime_len >= "41" & all_ds$anime_len < "60" ] = "41-60"

sqldf("Select anime_len, count(1) as cnt
      from all_ds
      group by anime_len")

```

#Related
Categorize related
```{r}
sqldf("SELECT substr(related, 1,10) as first10, count(1)
      FROM all_ds
      GROUP by first10")
all_ds$related_by = substr(all_ds$related, 1,10)
all_ds$related_by[all_ds$related_by == "{'Adaptati"] = "Adaptations"
all_ds$related_by[all_ds$related_by == "{'Parent s"] = "Relatives"
all_ds$related_by[all_ds$related_by == "{'Prequel'"] = "Relatives"
all_ds$related_by[all_ds$related_by == "{'Sequel':"] = "Relatives"
all_ds$related_by[all_ds$related_by == "{'Alternat"] = "Alternatives"
all_ds$related_by[all_ds$related_by == "{'Spin-off"] = "Alternatives"
all_ds$related_by[all_ds$related_by == "{'Side sto"] = "Alternatives"
all_ds$related_by[all_ds$related_by == "{'Other': "] = "Other"
all_ds$related_by[all_ds$related_by == "{'Full sto"] = "Other"
all_ds$related_by[all_ds$related_by == "{'Summary'"] = "Other"
all_ds$related_by[all_ds$related_by == "{'Characte"] = "Other"
all_ds$related_by[all_ds$related_by == "[]"] = "Other"
all_ds$related_by[all_ds$related_by == ""] = "Other"


sqldf("select related_by, count(1) as cnt
      from all_ds
      group by related_by
      order by cnt desc")
```

Licensor
Categorize Licensor
```{r}

```



Studio
Categorize Studio
```{r}

```

My_score
Categorize my_score into high and low
```{r}
all_ds$my_scr_bin = all_ds$my_score
all_ds$my_scr_bin = ifelse((all_ds$my_score) == "8" | (all_ds$my_score) == "9" | (all_ds$my_score) == "10", "high", "low")

table(all_ds$my_scr_bin)




```


```{r}
colnames(all_ds)

final_ds = all_ds[, c("my_scr_bin", "type", "anime_src", "episodes", "anime_len", "rating", "score", "scored_by", "rank", "popularity", "members", "favorites", "broadcast", "producer", "licensor", "studio", "my_watched_episodes", "my_status", "user_watching", "user_completed", "user_onhold", "user_dropped", "user_plantowatch", "lst_online_yr", "join_yr", "gnd_grp", "related_by")]

head(final_ds)


```

Random Forest
```{r}

```

